{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neurônio Sigmóide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções auxiliares\n",
    "\n",
    "Estas funções servem para ajudar a analisar o funcionamento do perceptron. Em especial, a função acc_perceptron abaixo retorna a acurácia do perceptron em um conjunto de dados X e rótulos Y. A acurácia é dada por:\n",
    "\n",
    "$$acc = \\frac{\\text{acertos}}{\\text{acertos}+\\text{erros}}$$\n",
    "\n",
    "Desta forma a acurácia é um número que vai de $0$ a $1$ e indica a proporção de acertos em relação ao número total de exemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plota as características feature_idxs (lista com 2 indíces de características)\n",
    "#X é um vetor onde todos os elementos antes do sep_idxs são da classe 1 e \n",
    "#todos de sep_idxs + 1 até o fim são da classe 2.\n",
    "def plot_features(X, sep_idx, feature_idxs):\n",
    "    c1 = 'red'\n",
    "    c2 = 'green'\n",
    "    \n",
    "    f1_a = []\n",
    "    f2_a = []\n",
    "    f1_b = []\n",
    "    f2_b = []\n",
    "    \n",
    "    for i in X[:sep_idx]:\n",
    "        f1_a.append(i[feature_idxs[0]])\n",
    "        f2_a.append(i[feature_idxs[1]])\n",
    "    \n",
    "    for i in X[sep_idx + 1:]:\n",
    "        f1_b.append(i[feature_idxs[0]])\n",
    "        f2_b.append(i[feature_idxs[1]])   \n",
    "        \n",
    "    plt.scatter(f1_a, f2_a, c=c1)\n",
    "    plt.scatter(f1_b, f2_b, c=c2)\n",
    "\n",
    "#Retorna a acurácia do perceptron. X e Y são vetores paralelos e w é o vetor de pesos.\n",
    "def acc_perceptron(X, Y, w):\n",
    "    hits = float(0)\n",
    "    misses = float(0)\n",
    "    for n in xrange(len(X)):\n",
    "        x_n = X[n]\n",
    "        d_n = d(Y, n)\n",
    "        y_n = decision(eval_sigmoid_neuron(x_n, w))\n",
    "        if (d_n - y_n) == 0:\n",
    "            hits+=1\n",
    "        else:\n",
    "            misses+=1\n",
    "            \n",
    "    return hits/(hits+misses)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrição do Neurônio Sigmóide\n",
    "\n",
    "O neurônio sigmóide difere do perceptron nos seguintes aspectos:\n",
    "\n",
    "1. A saída pode assumir valores entre 0 e 1.\n",
    "\n",
    "2. Usa a função de ativação sigmóide (também conhecida como função logística):\n",
    "\n",
    "    $$\\sigma(v) = \\frac{1}{1 + e^{-v}} = \\frac{1}{1 + \\exp \\left ( - \\sum_j w_j x_j \\right )} $$\n",
    "\n",
    "    ![Gráfico da função sigmoide e de sua primeira derivada](sigmoid.png)\n",
    "    \n",
    "    Note sua semelhança com a função *threshold* (limiar), exceto que deslocada \"para cima\" e com uma suavização entre -4 e 4, aproximadamente. Essa suavização nos permite calcular a derivada da função, que é útil para o processo de otimização com a descida do gradiente. Assim podemos calcular as mudanças na saída a partir de pequenas variações dos pesos sinápticos do neurônio.\n",
    "\n",
    "    Desta forma, a saída do neurônio sigmóide é dado por:\n",
    "    \n",
    "    $$\\hat y = \\sigma(x\\cdot w)$$\n",
    "    \n",
    "3. O mapeamento da saída do neurônio sigmóide em classes (em problemas de classificação) é feita de forma arbitrária. Por exemplo, a decisão final pode ser dada por uma função por partes:\n",
    "\n",
    "$$ \\text{decision}(v) =\n",
    "  \\begin{cases}\n",
    "    -1       & \\quad \\text{if } v \\leq 0.5\\\\\n",
    "    1  & \\quad \\text{if } v \\gt 0.5\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "### Treinamento com a descida do gradiente\n",
    "\n",
    "Podemos definir uma função de erro (também chamada de Função de Custo) para quantificar os erros de predição para um conjunto de N exemplos:\n",
    "\n",
    "$$C = \\frac{1}{2N} \\sum_{i=1}^{N} (\\hat y_i - y_i)^2$$\n",
    "\n",
    "tal que $\\hat y_i = \\sigma(x(i)\\cdot w(k))$ é a predição correspondente à saída do neurônio sigmóide para o exemplo $i$, na época $k$ e $y_i$ é a saída desejada (gabarito) do exemplo $i$.\n",
    "\n",
    "O que queremos é o vetor de pesos $w^*$ que minimiza a função de custo C:\n",
    "\n",
    "$$\\arg \\min_{w^*} C$$\n",
    "\n",
    "Um dos métodos usados para minimizar esta função de custo é a descida do gradiente. Lembrando que o passo da descida do gradiente é dado:\n",
    "\n",
    "$$w(n+1) = w(n) - \\epsilon \\nabla_w f(w)$$ \n",
    "\n",
    "tal que $\\nabla_w f(w)$ é o vetor gradiente da função f(w), a qual se deseja minimizar.\n",
    "\n",
    "Desta forma, para minimizar C, é necessário calcular seu gradiente, $\\nabla_wC$. Note que $C$ é uma função que depende indiretamente de $w$, uma vez que $\\hat y$ é uma função de $w$. Desta forma, podemos usar a regra da cadeia para computar $\\nabla_wC$. Sem perda de generalizade, usando a regra da cadeia, o cálculo da derivada parcial de C em função de $w_0$ é dada por:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w_0} = \\frac{\\partial C}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial w_0}$$.\n",
    "\n",
    "Aplicando a regra do \"tomba\":\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial \\hat y} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat y_i - y_i)$$\n",
    "\n",
    "Aplicando a regra do quociente:\n",
    "\n",
    "$$\\frac{\\partial \\hat y}{\\partial w_0} = \\frac{\\partial \\sigma(\\hat y)}{\\partial w_0} = \\frac{\\partial \\frac{1}{1 + \\exp(-(x \\cdot w))}}{\\partial w_0} = \\frac{e^{- \\hat y}}{(1+e^{- \\hat y})^2} x_0 $$\n",
    "\n",
    "Desta forma, \n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w_0} = \\frac{\\partial C}{\\partial \\hat y} \\frac{e^{- \\hat y}}{(1+e^{- \\hat y})^2} x_0 $$.\n",
    "\n",
    "Seja \n",
    "\n",
    "$$\\sigma'(\\hat y) = \\frac{e^{- \\hat y}}{(1+e^{- \\hat y})^2} $$\n",
    "\n",
    "Desta forma, $\\nabla_w C$ é dado por:\n",
    "\n",
    "$$\\nabla_w C = \\left \\{ \\frac{\\partial C}{\\partial \\hat y} \\sigma'(\\hat y) x_0, \\frac{\\partial C}{\\partial \\hat y} \\sigma'(\\hat y) x_1, \\dots, \\frac{\\partial C}{\\partial \\hat y} \\sigma'(\\hat y) x_m \\right \\}$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função de decisão: arbitrariamente: x > 0.5 é classe 1 e x <= 0.5 é classe 0\n",
    "def decision(x):\n",
    "    return 1 if x > 0.5 else 0\n",
    "\n",
    "#Função d que indica a saída desejada.\n",
    "def d(targets, idx):\n",
    "    return targets[idx]\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_dx(x):\n",
    "    return np.exp(-x) / (1 + np.exp(-x))**2\n",
    "\n",
    "#Avaliação do neurônio sigmóide\n",
    "def eval_sigmoid_neuron(x, w):\n",
    "    return sigmoid(np.dot(x,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bancada de Testes do Neurônio Sigmóide (Treino online)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treinamento do neurônio sigmóide\n",
    "def fit_neuron(X, Y, eta, shuffle=True,w_inicial=None):\n",
    "    if shuffle:\n",
    "        s= np.arange(len(X))\n",
    "        np.random.shuffle(s)\n",
    "        X = X[s]\n",
    "        Y = Y[s]\n",
    "    \n",
    "    #inicializar vetor de pesos com todos os valores em 0 caso um w_inicial não seja passado.\n",
    "    w = w_inicial if w_inicial is not None else np.zeros(X.shape[1])\n",
    "    \n",
    "    for k in xrange(1):\n",
    "        for n in xrange(len(X)):\n",
    "            x_n = X[n]\n",
    "            y_n = eval_sigmoid_neuron(x_n, w)\n",
    "            grad = (y_n - d(Y, n)) * (sigmoid_dx(np.dot(x_n,w)) * x_n)\n",
    "            w = w - eta * grad        \n",
    "    return w\n",
    "\n",
    "dataset = load_iris()\n",
    "\n",
    "X = dataset.data\n",
    "Y = dataset.target\n",
    "\n",
    "#adicionar a coluna de 1's (bias)\n",
    "X = np.insert(X, 0, np.array([1] * len(X)), axis=1)\n",
    "\n",
    "#usar os 100 primeiros exemplos e 2 primeiras características (e o bias na primeira coluna).\n",
    "#Escolhi os 100 primeiros pq os 50 primeiros são da classe 0 e os próximos 50 são da classe 1.\n",
    "X = X[:100,:3]\n",
    "#recuperar também os rótulos dos 100 primeiros exemplos\n",
    "Y = Y[:100]\n",
    "\n",
    "#Treinar o perceptron\n",
    "w = fit_neuron(X, Y, eta=0.1, shuffle=True)\n",
    "print('vetor de pesos apos o treino: %s' % str(w))\n",
    "\n",
    "#plotar as features!\n",
    "plot_features(X, 50, [1,2])\n",
    "\n",
    "#plotar a fronteira de decisão\n",
    "xdb = np.linspace(np.min(X[:,1]),np.max(X[:,1]),20)\n",
    "ydb = map(lambda x: (-w[1]/w[2])*x + (-w[0]/w[2]), xdb)\n",
    "plt.plot(xdb, ydb)\n",
    "\n",
    "print('Acurácia: %.2f' % acc_perceptron(X, Y, w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = fit_neuron(X, Y, eta=0.1, shuffle=True, w_inicial=w)\n",
    "print w\n",
    "#plotar as features!\n",
    "plot_features(X, 50, [1,2])\n",
    "\n",
    "#plotar a fronteira de decisão\n",
    "xdb = np.linspace(np.min(X[:,1]),np.max(X[:,1]),20)\n",
    "ydb = map(lambda x: (-w[1]/w[2])*x + (-w[0]/w[2]), xdb)\n",
    "plt.plot(xdb, ydb)\n",
    "\n",
    "print('Acurácia: %.2f' % acc_perceptron(X, Y, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização do treino passo-a-passo (Treino online)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta parte faz o treinamento e grava todos os passos (valor do vetor de pesos) do algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treinamento do neurônio sigmóide\n",
    "def fit_neuron_steps(X, Y, eta, epochs=1, shuffle=True,w_inicial=None):\n",
    "    steps = []\n",
    "    \n",
    "    w = w_inicial if w_inicial is not None else np.zeros(X.shape[1])\n",
    "    \n",
    "    for e in range(epochs):\n",
    "    \n",
    "        if shuffle:\n",
    "            s= np.arange(len(X))\n",
    "            np.random.shuffle(s)\n",
    "            X = X[s]\n",
    "            Y = Y[s]\n",
    "\n",
    "        #inicializar vetor de pesos com todos os valores em 0 caso um w_inicial não seja passado.\n",
    "        for k in xrange(1):\n",
    "            for n in xrange(len(X)):\n",
    "                x_n = X[n]\n",
    "                y_n = eval_sigmoid_neuron(x_n, w)\n",
    "                grad = (y_n - d(Y, n)) * (sigmoid_dx(np.dot(x_n,w)) * x_n)\n",
    "                w = w - eta * grad  \n",
    "                steps.append(w)\n",
    "                \n",
    "    return steps\n",
    "\n",
    "steps =  fit_neuron_steps(X,Y,0.4, epochs=10, shuffle=True, w_inicial=None)\n",
    "\n",
    "print len(steps)\n",
    "\n",
    "si = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a caixa abaixo sucessivamente para ver a trajetória do vetor de pesos conforme as variações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = steps[si]\n",
    "print 'Passo %d: ' % si, w\n",
    "#plotar as features!\n",
    "plot_features(X, 50, [1,2])\n",
    "\n",
    "#plotar a fronteira de decisão\n",
    "xdb = np.linspace(np.min(X[:,1]),np.max(X[:,1]),20)\n",
    "ydb = map(lambda x: (-w[1]/w[2])*x + (-w[0]/w[2]), xdb)\n",
    "plt.plot(xdb, ydb)\n",
    "\n",
    "print('Acurácia: %.2f' % acc_perceptron(X, Y, w))\n",
    "\n",
    "si = (si + 20) % len(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bancada de Testes do Neurônio Sigmóide (Treino em lote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treinamento do neurônio sigmóide\n",
    "def fit_neuron_batch(X, Y, eta, epochs=5, shuffle=True,w_inicial=None):\n",
    "    \n",
    "    batch_size=40\n",
    "    \n",
    "    if shuffle:\n",
    "        s= np.arange(len(X))\n",
    "        np.random.shuffle(s)\n",
    "        X = X[s]\n",
    "        Y = Y[s]\n",
    "    \n",
    "    #inicializar vetor de pesos com todos os valores em 0 caso um w_inicial não seja passado.\n",
    "    w = w_inicial if w_inicial is not None else np.zeros(X.shape[1])\n",
    "    eqms = []\n",
    "    \n",
    "    for k in xrange(epochs):\n",
    "        for b in xrange(0, len(X)-batch_size, batch_size):\n",
    "            grad = 0\n",
    "            eqm = 0\n",
    "            for n in range(b, b + batch_size):\n",
    "                x_n = X[n]\n",
    "                y_n = eval_sigmoid_neuron(x_n, w)\n",
    "                eqm+= (y_n - d(Y, n))**2 \n",
    "                grad += (y_n - d(Y, n)) * (sigmoid_dx(np.dot(x_n,w)) * x_n)\n",
    "            \n",
    "            grad = grad / batch_size\n",
    "            w = w - eta * grad\n",
    "            eqms.append(eqm / (2 * batch_size))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(eqms)\n",
    "    plt.xlabel('Numero de Batches')\n",
    "    plt.ylabel('EQM')\n",
    "        \n",
    "    return w\n",
    "\n",
    "dataset = load_iris()\n",
    "\n",
    "X = dataset.data\n",
    "Y = dataset.target\n",
    "\n",
    "#adicionar a coluna de 1's (bias)\n",
    "X = np.insert(X, 0, np.array([1] * len(X)), axis=1)\n",
    "\n",
    "#usar os 100 primeiros exemplos e 2 primeiras características (e o bias na primeira coluna).\n",
    "#Escolhi os 100 primeiros pq os 50 primeiros são da classe 0 e os próximos 50 são da classe 1.\n",
    "X = X[:100,:3]\n",
    "#recuperar também os rótulos dos 100 primeiros exemplos\n",
    "Y = Y[:100]\n",
    "\n",
    "#Treinar o perceptron\n",
    "w = fit_neuron_batch(X, Y, eta=0.1, epochs=100, shuffle=True)\n",
    "print('vetor de pesos apos o treino: %s' % str(w))\n",
    "\n",
    "#plotar as features!\n",
    "plt.figure()\n",
    "plot_features(X, 50, [1,2])\n",
    "\n",
    "#plotar a fronteira de decisão\n",
    "xdb = np.linspace(np.min(X[:,1]),np.max(X[:,1]),20)\n",
    "ydb = map(lambda x: (-w[1]/w[2])*x + (-w[0]/w[2]), xdb)\n",
    "plt.plot(xdb, ydb)\n",
    "\n",
    "print('Acurácia: %.2f' % acc_perceptron(X, Y, w))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
