{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n",
    "O perceptron, mais especificamente o Perceptron de Rosenblatt, é um dispositivo computacional capaz de classificar padrões linearmente separáveis. Formalmente, a saída do perceptron é calculada por:\n",
    "\n",
    "$$y = \\text{signum}(w \\cdot x )$$\n",
    "\n",
    "tal que $x = \\{+1, x_1, x_2, \\dots, x_m\\}$ é um vetor de entrada com a primeira entrada fixada em +1 e $w = \\{w_0, w_1, w_2, \\dots, w_m \\}$ é o vetor de pesos sinápticos. Signum é uma função quantizadora que mapeia valores positivos para 1 e valores negativos para -1:\n",
    "\n",
    "$$ \\text{signum}(v) =\n",
    "  \\begin{cases}\n",
    "    -1       & \\quad \\text{if } v \\leq 0\\\\\n",
    "    1  & \\quad \\text{if } v \\gt 0\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "Logo, o perceptron classifica o vetor de entrada $x$ em uma entre duas classes possíveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções auxiliares\n",
    "\n",
    "Estas funções servem para ajudar a analisar o funcionamento do perceptron. Em especial, a função acc_perceptron abaixo retorna a acurácia do perceptron em um conjunto de dados X e rótulos Y. A acurácia é dada por:\n",
    "\n",
    "$$acc = \\frac{\\text{acertos}}{\\text{acertos}+\\text{erros}}$$\n",
    "\n",
    "Desta forma a acurácia é um número que vai de $0$ a $1$ e indica a proporção de acertos em relação ao número total de exemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plota as características feature_idxs (lista com 2 indíces de características)\n",
    "#X é um vetor onde todos os elementos antes do sep_idxs são da classe 1 e \n",
    "#todos de sep_idxs + 1 até o fim são da classe 2.\n",
    "def plot_features(X, sep_idx, feature_idxs):\n",
    "    c1 = 'red'\n",
    "    c2 = 'green'\n",
    "    \n",
    "    f1_a = []\n",
    "    f2_a = []\n",
    "    f1_b = []\n",
    "    f2_b = []\n",
    "    \n",
    "    for i in X[:sep_idx]:\n",
    "        f1_a.append(i[feature_idxs[0]])\n",
    "        f2_a.append(i[feature_idxs[1]])\n",
    "    \n",
    "    for i in X[sep_idx + 1:]:\n",
    "        f1_b.append(i[feature_idxs[0]])\n",
    "        f2_b.append(i[feature_idxs[1]])   \n",
    "        \n",
    "    plt.scatter(f1_a, f2_a, c=c1)\n",
    "    plt.scatter(f1_b, f2_b, c=c2)\n",
    "\n",
    "#Retorna a acurácia do perceptron. X e Y são vetores paralelos e w é o vetor de pesos.\n",
    "def acc_perceptron(X, Y, w):\n",
    "    hits = float(0)\n",
    "    misses = float(0)\n",
    "    for n in xrange(len(X)):\n",
    "        x_n = X[n]\n",
    "        d_n = d(Y, n)\n",
    "        y_n = eval_perceptron(x_n, w)\n",
    "        if (d_n - y_n) == 0:\n",
    "            hits+=1\n",
    "        else:\n",
    "            misses+=1\n",
    "            \n",
    "    return hits/(hits+misses)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação do Perceptron\n",
    "\n",
    "As funções **signum** e **eval_perceptron** estão descritas no início deste caderno. A função **d** indica a saída desejada do vetor indexado por *idx* em *targets*. Nesta implementação, a classe com rótulo 0 é considerada positiva, enquanto as classes com outros rótulos são consideradas negativas.\n",
    "\n",
    "### Perceptron Convergence\n",
    "\n",
    "É o algoritmo online de aprendizagem do perceptron. Recebe uma matriz de características X, onde cada linha é um exemplo e cada coluna uma característica; um vetor de rótulos Y, paralelo ao vetor X; $0 \\le \\eta \\leq 1$ (eta), a taxa de aprendizagem; e *shuffle*, um parametro que indica se é necessário embaralhar o vetor X e Y paralelamente antes do treino.\n",
    "\n",
    "Este algoritmo é equivalente à descida do gradiente em relação à função de threshold. No entanto, vamos ver a razão disso mais pra frente. A idéia é atualizar os pesos apenas se houver um erro na predição. a atualização é realizada da seguinte forma:\n",
    "\n",
    "$$w(n+1) = w(n) + \\eta (d(Y,n) - y(n)) x(n)$$\n",
    "\n",
    "Tal que $w(n)$ é o vetor de pesos na iteração $n$, $d(Y,n)$ é o valor desejado do perceptron na iteração $n$, $y(n)$ é a saída obtida pelo perceptron na iteração $n$ e $x(n)$ é o vetor de entrada na interação $n$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função signum, que quantiza a saída do perceptron pra 1 se x for positivo ou -1, caso contrário.\n",
    "def signum(x):\n",
    "    return 1 if x > 0 else -1\n",
    "\n",
    "#Função d que indica a saída desejada. Note a classe 0 está mapeada para 1, enquanto \n",
    "#as outras estão mapeadas para -1\n",
    "def d(targets, idx):\n",
    "    return 1 if targets[idx] == 0 else -1\n",
    "\n",
    "#Função do perceptron threshold com função signum\n",
    "def eval_perceptron(x, w):\n",
    "    return signum(np.dot(w,x))\n",
    "\n",
    "#convergência do perceptron.\n",
    "def perceptron_convergence(X, Y, eta, shuffle=True,w_inicial=None):\n",
    "    if shuffle:\n",
    "        s= np.arange(len(X))\n",
    "        np.random.shuffle(s)\n",
    "        X = X[s]\n",
    "        Y = Y[s]\n",
    "    \n",
    "    #inicializar vetor de pesos com todos os valores em 0 caso um w_inicial não seja passado.\n",
    "    w = w_inicial if w_inicial is not None else np.zeros(X.shape[1])\n",
    "    \n",
    "    for k in xrange(1):\n",
    "        for n in xrange(len(X)):\n",
    "            x_n = X[n]\n",
    "            y_n = eval_perceptron(x_n, w)\n",
    "            w = w + eta * (d(Y, n) - y_n)* x_n\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bancada de Testes do Perceptron\n",
    "\n",
    "Aqui é apresentada uma bancada de testes para o perceptron em um problema conhecido por ser linearmente separável.\n",
    "\n",
    "A base de dados usada é a Iris (do sklearn).\n",
    "\n",
    "### Traçando a Fronteira de Decisão\n",
    "\n",
    "Note que $y = w \\cdot x \\Leftrightarrow y = w_0x_0 + w_1x_1 + w_2x_2$, no caso que $m = 2$.\n",
    "\n",
    "Sabemos que a fronteira de decisão é a reta que passa exatamente quando $wx = 0$. Assim, sabemos que\n",
    "\n",
    "$$w_0x_0 + w_1x_1 + w_2x_2 = 0$$\n",
    "\n",
    "Isolando $x_1$, temos\n",
    "\n",
    "$$x_1 = \\frac{-w_0 - w_2x_2}{w_1}$$\n",
    "\n",
    "Assim, a reta cruza o eixo $x_1$ ($x_2 = 0$):\n",
    "\n",
    "$$Xx_1 = \\frac{-w0}{w1}$$\n",
    "\n",
    "da mesma forma, a reta cruza o eixo $x_2$ ($x_1 = 0$)\n",
    "\n",
    "$$Xx_2 = \\frac{-w0}{w2}$$\n",
    "\n",
    "Assim, sabemos que os pontos $P1 = \\left(\\frac{-w0}{w1}, 0 \\right)$ e $P2 = \\left(0, \\frac{-w0}{w2} \\right)$\n",
    "\n",
    "Podemos usar essa informação para montar a equação da reta correspondente. Primeiro basta encontrar o coeficiente angular usando P1 e P2:\n",
    "\n",
    "$$m = \\frac{P2y - P1y}{P2x-P2y}$$\n",
    "\n",
    "Tal que $x$ é a coordenada em $x_1$ e $y$ é a coordenada em $x_2$. Substituindo e expandindo:\n",
    "\n",
    "$$m = \\frac{-w1}{w2} $$\n",
    "\n",
    "Desta forma, $x_2$ em função de $x_1$ pode ser escrito como:\n",
    "\n",
    "$$x_2 = m x_1 + Xx_2 $$\n",
    "\n",
    "$$x_2 = \\frac{-w_1}{w_2}x_1 + \\frac{-w_0}{w_2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_iris()\n",
    "\n",
    "X = dataset.data\n",
    "Y = dataset.target\n",
    "\n",
    "#adicionar a coluna de 1's (bias)\n",
    "X = np.insert(X, 0, np.array([1] * len(X)), axis=1)\n",
    "\n",
    "#usar os 100 primeiros exemplos e 2 primeiras características (e o bias na primeira coluna).\n",
    "#Escolhi os 100 primeiros pq os 50 primeiros são da classe 0 e os próximos 50 são da classe 1.\n",
    "X = X[:100,:3]\n",
    "#recuperar também os rótulos dos 100 primeiros exemplos\n",
    "Y = Y[:100]\n",
    "\n",
    "#Treinar o perceptron\n",
    "w = perceptron_convergence(X, Y, eta=0.1)\n",
    "print('vetor de pesos apos o treino: %s' % str(w))\n",
    "\n",
    "#plotar as features!\n",
    "plot_features(X, 50, [1,2])\n",
    "\n",
    "#plotar a fronteira de decisão\n",
    "xdb = np.linspace(np.min(X[:,1]),np.max(X[:,1]),20)\n",
    "ydb = map(lambda x: (-w[1]/w[2])*x + (-w[0]/w[2]), xdb)\n",
    "plt.plot(xdb, ydb)\n",
    "\n",
    "print('Acurácia: %.2f' % acc_perceptron(X, Y, w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = perceptron_convergence(X, Y, eta=0.1, w_inicial=w)\n",
    "#plotar as features!\n",
    "plot_features(X, 50, [1,2])\n",
    "\n",
    "#plotar a fronteira de decisão\n",
    "xdb = np.linspace(np.min(X[:,1]),np.max(X[:,1]),20)\n",
    "ydb = map(lambda x: (-w[1]/w[2])*x + (-w[0]/w[2]), xdb)\n",
    "plt.plot(xdb, ydb)\n",
    "\n",
    "print('Acurácia: %.2f' % acc_perceptron(X, Y, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução do Perceptron (passo-a-passo)\n",
    "\n",
    "Veja a execução do perceptron passo-a-passo. Primeiramente execute a célula logo abaixo dessa. Isso irá executar o treino do perceptron e retornar todas as atualizações realizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_convergence_steps(X, Y, eta, shuffle=True):\n",
    "    #inicializar vetor de pesos com todos os valores em 0\n",
    "    if shuffle:\n",
    "        s= np.arange(len(X))\n",
    "        np.random.shuffle(s)\n",
    "        X = X[s]\n",
    "        Y = Y[s]\n",
    "        \n",
    "    w = np.zeros(X.shape[1])\n",
    "    all_w = [w]\n",
    "    for n in xrange(len(X)):\n",
    "        x_n = X[n]\n",
    "        y_n = eval_perceptron(x_n, w)\n",
    "        w = w + eta * (d(Y, n) - y_n)* x_n\n",
    "        if (d(Y,n) - y_n) != 0:\n",
    "            all_w.append(w)\n",
    "                \n",
    "\n",
    "    return all_w\n",
    "                \n",
    "dataset = load_iris()\n",
    "\n",
    "X = dataset.data\n",
    "Y = dataset.target\n",
    "\n",
    "#adicionar a coluna de 1's (bias)\n",
    "X = np.insert(X, 0, np.array([1] * len(X)), axis=1)\n",
    "\n",
    "X = X[:100,:3]\n",
    "Y = Y[:100]\n",
    "\n",
    "min_x = np.min(X[:,1])\n",
    "max_x = np.max(X[:,1])\n",
    "\n",
    "ws = perceptron_convergence_steps(X, Y, 0.001)\n",
    "i = 0\n",
    "print('Foram executados %d passos de atualização dos pesos' % len(ws))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após executar a célula acima uma vez, execute a célula abaixo várias vezes para ver como a fronteira de decisão muda conforme os pesos são atualizados!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute esta célula e veja as iterações do algoritmo de treinamento do perceptron.\n",
    "print('passo %d' % i)\n",
    "w = ws[i]\n",
    "\n",
    "plot_features(X, 50, [1,2])\n",
    "\n",
    "xdb = np.linspace(min_x, max_x,20)\n",
    "\n",
    "ydb = map(lambda x: (-w[1]/w[2])*x + (-w[0]/w[2]), xdb)\n",
    "\n",
    "plt.plot(xdb, ydb)\n",
    "plt.xlim(min_x,max_x)\n",
    "\n",
    "i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergência do *Perceptron* em Lote (*Batch*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_convergence_batch(X, Y, eta, shuffle=True,w_inicial=None):\n",
    "    if shuffle:\n",
    "        s= np.arange(len(X))\n",
    "        np.random.shuffle(s)\n",
    "        X = X[s]\n",
    "        Y = Y[s]\n",
    "    \n",
    "    #inicializar vetor de pesos com todos os valores em 0 caso um w_inicial não seja passado.\n",
    "    w = w_inicial if w_inicial is not None else np.zeros(X.shape[1])\n",
    "    w_deltas = []\n",
    "    for k in xrange(1):\n",
    "        for n in xrange(len(X)):\n",
    "            x_n = X[n]\n",
    "            y_n = eval_perceptron(x_n, w)\n",
    "            w_deltas.append(eta * (d(Y, n) - y_n)* x_n)\n",
    "    \n",
    "    w_deltas = np.array(w_deltas)\n",
    "        \n",
    "    return w + np.mean(w_deltas, axis=0)\n",
    "\n",
    "dataset = load_iris()\n",
    "\n",
    "X = dataset.data\n",
    "Y = dataset.target\n",
    "\n",
    "#adicionar a coluna de 1's (bias)\n",
    "X = np.insert(X, 0, np.array([1] * len(X)), axis=1)\n",
    "\n",
    "#usar os 100 primeiros exemplos e 2 primeiras características (e o bias na primeira coluna).\n",
    "#Escolhi os 100 primeiros pq os 50 primeiros são da classe 0 e os próximos 50 são da classe 1.\n",
    "X = X[:100,:3]\n",
    "#recuperar também os rótulos dos 100 primeiros exemplos\n",
    "Y = Y[:100]\n",
    "\n",
    "#Treinar o perceptron\n",
    "w = perceptron_convergence_batch(X, Y, eta=0.01)\n",
    "print('vetor de pesos apos o treino: %s' % str(w))\n",
    "\n",
    "#plotar as features!\n",
    "plot_features(X, 50, [1,2])\n",
    "\n",
    "#plotar a fronteira de decisão\n",
    "xdb = np.linspace(np.min(X[:,1]),np.max(X[:,1]),20)\n",
    "ydb = map(lambda x: (-w[1]/w[2])*x + (-w[0]/w[2]), xdb)\n",
    "plt.plot(xdb, ydb)\n",
    "\n",
    "print('Acurácia: %.2f' % acc_perceptron(X, Y, w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = perceptron_convergence_batch(X, Y, eta=0.01, w_inicial=w)\n",
    "print('vetor de pesos apos o treino: %s' % str(w))\n",
    "#plotar as features!\n",
    "plot_features(X, 50, [1,2])\n",
    "\n",
    "#plotar a fronteira de decisão\n",
    "xdb = np.linspace(np.min(X[:,1]),np.max(X[:,1]),20)\n",
    "ydb = map(lambda x: (-w[1]/w[2])*x + (-w[0]/w[2]), xdb)\n",
    "plt.plot(xdb, ydb)\n",
    "\n",
    "print('Acurácia: %.2f' % acc_perceptron(X, Y, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
