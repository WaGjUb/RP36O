{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descida do Gradiente\n",
    "\n",
    "O exemplo abaixo mostra computação numérica da minimização de uma função univariada usando o método da descida do gradiente. O novo ponto $x'$ do método da descida do gradiente é dado por:\n",
    "\n",
    "$$x' = x  - \\epsilon \\; f'(x)$$\n",
    "\n",
    "Tal que $\\epsilon$ é a taxa de aprendizagem, $f'(x)$ é a primeira derivada de $f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f:  a função sendo minimizada\n",
    "#ff: a derivada da f\n",
    "\n",
    "#def f(x): return x**3 + x**2 - 10\n",
    "#def ff(x): return 3*(x**2) + 2*x\n",
    "\n",
    "def f(x): return 2*(x**2) + 3*x + 5\n",
    "def ff(x): return 4*x + 3\n",
    "\n",
    "inicial = random.uniform(-0.5,10)\n",
    "epsilon = 10**-1\n",
    "maxIters = 1000\n",
    "x = inicial\n",
    "minimo = inicial\n",
    "\n",
    "print \"x inicial = \", x\n",
    "i = 0\n",
    "while i < maxIters:\n",
    "    print i, x\n",
    "    x = x - epsilon * ff(x)\n",
    "#     df = ff(x)\n",
    "#     if df < 0:\n",
    "#         x+=epsilon\n",
    "#     else:\n",
    "#         if df > 0:\n",
    "#             x-=epsilon\n",
    "#         else:\n",
    "#             break\n",
    "    \n",
    "    if f(x) < f(minimo):\n",
    "        minimo = x\n",
    "        \n",
    "    if ff(x) < 10e-5:\n",
    "        break\n",
    "        \n",
    "    i+=1\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "ffx = []\n",
    "ffy = []\n",
    "    \n",
    "for i in [p for p in np.linspace(-10,10,100)]:\n",
    "    x.append(i)\n",
    "    y.append(f(i))\n",
    "    ffx.append(i)\n",
    "    ffy.append(ff(i))\n",
    "\n",
    "plt.plot(x,y, label='f(x)')\n",
    "plt.plot(ffx,ffy, label='Fx(f(x))')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()    \n",
    "    \n",
    "    \n",
    "    \n",
    "print \"minimo encontrado: f(\", minimo, \") = \", f(minimo)\n",
    "\n",
    "#Plotar um \"zoom\" do resultado na função otimizada e no seu gradiente\n",
    "x = []\n",
    "y = []\n",
    "ffx = []\n",
    "ffy = []\n",
    "for i in [p for p in np.linspace(math.floor(minimo)-3,math.floor(minimo)+3,100)]:\n",
    "    x.append(i)\n",
    "    y.append(f(i))\n",
    "    ffx.append(i)\n",
    "    ffy.append(ff(i))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x,y, label='f(x)')\n",
    "plt.plot(ffx,ffy, label='Fx(f(x))')\n",
    "plt.plot(minimo, f(minimo), marker='x', color='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descida do Gradiente Multivariado\n",
    "\n",
    "O exemplo abaixo mostra a computação numérica da minimização de $f(x,y)$. Utiliza o método da descida do gradiente. O novo ponto $x'$ do método do gradiente descendente é dado por:\n",
    "\n",
    "$$x' = x - \\epsilon \\nabla_x f(x)$$\n",
    "\n",
    "Tal que $\\epsilon$ é a taxa de aprendizado, ou passo, e $\\nabla_x f(x)$ é o vetor gradiente de $f(x)$.\n",
    "\n",
    "O vetor gradiente $\\nabla_x f(x)$ é dado por:\n",
    "\n",
    "$$\\nabla_x f(x) = \\left \\{ \\frac{\\partial f(x)}{\\partial x_0}, \\frac{\\partial f(x)}{\\partial x_1}, \\dots, \\frac{\\partial f(x)}{ \\partial x_m} \\right \\}.$$\n",
    "\n",
    "A implementação está simplificada por uma questão de facilitar o entendimento do conceito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f:  a função sendo minimizada\n",
    "#fx: a derivada parcial de f em relação a x\n",
    "#fy: a derivada parcial de f em relação a y\n",
    "\n",
    "#def f(x,y): return math.sin(x*y)\n",
    "#def fy(x,y): return x * math.cos(x*y)\n",
    "#def fx(x,y): return y * math.cos(x*y)\n",
    "\n",
    "def f(x,y):  return (x**2)/5 + (y**2)/10\n",
    "def fx(x,y): return (2*x) /5\n",
    "def fy(x,y): return y / 5\n",
    "\n",
    "xinicial = random.uniform(-10,10)\n",
    "yinicial = random.uniform(-10,10)\n",
    "epsilon = 10**-1\n",
    "maxIters = 1000\n",
    "x = xinicial\n",
    "y = yinicial\n",
    "xminimo = xinicial\n",
    "yminimo = yinicial\n",
    "\n",
    "print \"x inicial = \", x, \"y inicial = \", y\n",
    "i = 0\n",
    "while i < maxIters:\n",
    "    #x' = x - (eps * grad(f(x)))\n",
    "    \n",
    "    print i, x, y\n",
    "    \n",
    "    dy = fy(x,y)\n",
    "    dx = fx(x,y)\n",
    "    x = x - (epsilon * dx)\n",
    "    y = y - (epsilon * dy)\n",
    "    \n",
    "    if f(x,y) < f(xminimo,yminimo):\n",
    "        xminimo = x\n",
    "        yminimo = y\n",
    "        \n",
    "    i+=1\n",
    "\n",
    "print \"minimo encontrado: f(\", xminimo, \",\" , yminimo, \") = \", f(xminimo, yminimo)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
